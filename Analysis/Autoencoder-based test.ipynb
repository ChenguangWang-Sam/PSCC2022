{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25fe704a",
   "metadata": {},
   "source": [
    "# About this code\n",
    "\n",
    "- Author: Chenguang Wang   \n",
    "- Email: samwangchenguang@gmail.com\n",
    "- Project name: Generating multivariate load states using a (conditional) variational autoencoder\n",
    "- Motivation: This is a project for PSCC2022 – Power Systems Computation Conference: [Homepage of the conference](https://pscc2022.pt/)\n",
    "- Aim of this code: Analyze point-wise multivariate dependencies of load data\n",
    "- Reference Paper: You can check our paper for more details: [Link of the paper](https://arxiv.org/abs/2110.11435)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7bf679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function lab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd976aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data import\n",
    "Train_load_imported = pd.read_csv(\"../Data/13-17_32_Train.csv\", index_col=0)\n",
    "Test_load_imported = pd.read_csv(\"../Data/13-17_32_Test.csv\", index_col=0)\n",
    "\n",
    "σ_0_1_Time_all_mu_nor_imported = pd.read_csv(\"../Generations/σ'_0.1_Time_all_mu_nor.csv\", index_col=0)\n",
    "σ_0_1_Time_all_noisy_nor_imported = pd.read_csv(\"../Generations/σ'_0.1_Time_all_noisy_nor.csv\", index_col=0)\n",
    "σ_auto_Time_all_mu_nor_β_1_imported = pd.read_csv(\"../Generations/σ'_auto_Time_all_mu_nor_β=1.csv\", index_col=0)\n",
    "σ_auto_Time_all_noisy_nor_β_1_imported = pd.read_csv(\"../Generations/σ'_auto_Time_all_noisy_nor_β=1.csv\", index_col=0)\n",
    "\n",
    "country_number = 32\n",
    "\n",
    "# Data scale\n",
    "max_Tr = np.max(Train_load_imported.iloc[:,:country_number], axis = 0)\n",
    "min_Tr = np.min(Train_load_imported.iloc[:,:country_number], axis = 0)\n",
    "\n",
    "Train_load_scaled = (Train_load_imported.iloc[:,:country_number]-min_Tr)/(max_Tr-min_Tr)\n",
    "Test_load_scaled  = (Test_load_imported.iloc[:,:country_number]-min_Tr)/(max_Tr-min_Tr)\n",
    "\n",
    "# Data type change\n",
    "Train_load                      = Train_load_scaled.values.astype(np.float32)\n",
    "Test_load                       = Test_load_scaled.values.astype(np.float32)\n",
    "\n",
    "σ_0_1_Time_all_mu_nor           = σ_0_1_Time_all_mu_nor_imported.values.astype(np.float32)\n",
    "σ_0_1_Time_all_noisy_nor        = σ_0_1_Time_all_noisy_nor_imported.values.astype(np.float32)\n",
    "σ_auto_Time_all_mu_nor_β_1      = σ_auto_Time_all_mu_nor_β_1_imported.values.astype(np.float32)\n",
    "σ_auto_Time_all_noisy_nor_β_1   = σ_auto_Time_all_noisy_nor_β_1_imported.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b92575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Preparation (Neuro network dessign) =======================\n",
    "\n",
    "# ========================== Initialization fuction ============================\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "# ===================== Data dimension calcuation ==============================\n",
    "\n",
    "# Dimension of the input\n",
    "X_dim_all=np.shape(Train_load)\n",
    "X_dim = X_dim_all[1]\n",
    "# Dimension of the hidden layer\n",
    "h_dim_1 = 24\n",
    "# Dimension of the hidden layer\n",
    "h_dim_2 = 16\n",
    "# Dimension of the latent space\n",
    "z_dim = 8\n",
    "\n",
    "# ============================= Print the dimension ============================\n",
    "\n",
    "print ('The dimension of the input data is: ', X_dim)\n",
    "print ('The dimension of the hidden layer 1 is: ', h_dim_1)\n",
    "print ('The dimension of the hidden layer 2 is: ', h_dim_2)\n",
    "print ('The dimension of the latent space data is: ', z_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b358e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================ Evaluation by CDF using autoencoder =========================\n",
    "\n",
    "# Training condition: lr (learning rate)=0.0001; batch_size (mb_size) =64;\n",
    "#                     Epoch=200,000.\n",
    "\n",
    "# ========================== Training process ==================================\n",
    "\n",
    "# =============================== Encoder ======================================\n",
    "\n",
    "#Traning setting \n",
    "mb_size = 64 # Batch size\n",
    "lr = 0.0001 #learning rate\n",
    "\n",
    "# Data\n",
    "X = tf.placeholder(tf.float32, shape=[None, X_dim])\n",
    "z = tf.placeholder(tf.float32, shape=[None, z_dim])\n",
    "\n",
    "# Layer definitions\n",
    "En_W1 = tf.Variable(xavier_init([X_dim, h_dim_1]))\n",
    "En_b1 = tf.Variable(tf.zeros(shape=[h_dim_1]))\n",
    "\n",
    "En_W2 = tf.Variable(xavier_init([h_dim_1, h_dim_2]))\n",
    "En_b2 = tf.Variable(tf.zeros(shape=[h_dim_2]))\n",
    "\n",
    "En_W3 = tf.Variable(xavier_init([h_dim_2, z_dim]))\n",
    "En_b3 = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "# Network\n",
    "def En(X): \n",
    "    h1 = tf.nn.relu(tf.matmul(X, En_W1) + En_b1)\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, En_W2) + En_b2)\n",
    "    z = tf.matmul(h2, En_W3) + En_b3\n",
    "    return z\n",
    "\n",
    "# =============================== Decoder =======================================\n",
    "\n",
    "# Layer definitions\n",
    "De_W1 = tf.Variable(xavier_init([z_dim, h_dim_2]))  \n",
    "De_b1 = tf.Variable(tf.zeros(shape=[h_dim_2]))\n",
    "\n",
    "De_W2 = tf.Variable(xavier_init([h_dim_2, h_dim_1]))\n",
    "De_b2 = tf.Variable(tf.zeros(shape=[h_dim_1]))\n",
    "\n",
    "De_W3 = tf.Variable(xavier_init([h_dim_1, X_dim]))\n",
    "De_b3 = tf.Variable(tf.zeros(shape=[X_dim]))\n",
    "\n",
    "# Network\n",
    "def De(z): \n",
    "    h1 = tf.nn.relu(tf.matmul(z, De_W1) + De_b1)\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, De_W2) + De_b2)\n",
    "    Re_X = tf.matmul(h2, De_W3) + De_b3\n",
    "    return Re_X\n",
    "\n",
    "# ================================ Training ====================================\n",
    "\n",
    "# Training functions\n",
    "z = En(X) # Encoder \n",
    "Re_X = De(z) # Decoder\n",
    "\n",
    "# Training loss\n",
    "recon_loss= tf.reduce_mean((Re_X - X)**2)\n",
    "\n",
    "# Solver\n",
    "solver = tf.train.AdamOptimizer(lr).minimize(recon_loss)\n",
    "\n",
    "# Seesion\n",
    "sess = tf.Session() # Session definition\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for it in range(200000): # Training process\n",
    "    batch_index = np.random.choice(Train_load.shape[0], size=mb_size, replace=False)\n",
    "    _, loss = sess.run([solver, recon_loss], feed_dict={X: Train_load[batch_index]})\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('Loss_training_set: {:.4}'. format(loss))      \n",
    "        #Averagr Reconstructed generated data\n",
    "        Test_Re_er_mean = sess.run(recon_loss, feed_dict={X: Test_load})\n",
    "        print('Loss_testing_set: {:.4}'. format(Test_Re_er_mean))  \n",
    "        print()\n",
    "print(\"Training Finished!\") \n",
    "\n",
    "#Save the reslut    \n",
    "saver = tf.train.Saver() \n",
    "saver.save(sess, './filename.chkp') \n",
    "\n",
    "# Session Recover\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()                                                     \n",
    "saver.restore(sess, 'filename.chkp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef47bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================(1) Caculate the reconstruction errors=================\n",
    "# Reconstructed training data \n",
    "Train_load_Re_er = sess.run(Re_X, feed_dict={X: Train_load})\n",
    "# Mean square value of each training residual  \n",
    "Train_load_Re_er_list = np.mean((Train_load - Train_load_Re_er)**2, axis=1)\n",
    "\n",
    "# Reconstructed testing data \n",
    "Test_load_Re_er = sess.run(Re_X, feed_dict={X: Test_load})\n",
    "# Mean square value of each testing residual  \n",
    "Test_load_Re_er_list = np.mean((Test_load - Test_load_Re_er)**2, axis=1)\n",
    "\n",
    "# Reconstructed generated data （CVAE; auto σ'; noisy）\n",
    "σ_auto_Time_all_noisy_nor_β_1_Re_er = sess.run(Re_X, feed_dict={X: σ_auto_Time_all_noisy_nor_β_1}) \n",
    "# Mean square value of each generated residual\n",
    "σ_auto_Time_all_noisy_nor_β_1_Re_er_list = np.mean((σ_auto_Time_all_noisy_nor_β_1 - σ_auto_Time_all_noisy_nor_β_1_Re_er)**2, axis=1)\n",
    "\n",
    "# Reconstructed generated data (CVAE; σ'=0.1; noisy)\n",
    "σ_0_1_Time_all_noisy_nor_Re_er = sess.run(Re_X, feed_dict={X: σ_0_1_Time_all_noisy_nor}) \n",
    "# Mean square value of each generated residual\n",
    "σ_0_1_Time_all_noisy_nor_Re_er_list = np.mean((σ_0_1_Time_all_noisy_nor - σ_0_1_Time_all_noisy_nor_Re_er)**2, axis=1)\n",
    "\n",
    "# Reconstructed generated data (CVAE; auto σ'; Noise free)\n",
    "σ_auto_Time_all_mu_nor_β_1_Re_er = sess.run(Re_X, feed_dict={X: σ_auto_Time_all_mu_nor_β_1}) \n",
    "# Mean square value of each generated residual\n",
    "σ_auto_Time_all_mu_nor_β_1_Re_er_list = np.mean((σ_auto_Time_all_mu_nor_β_1 - σ_auto_Time_all_mu_nor_β_1_Re_er)**2, axis=1)\n",
    "\n",
    "# Reconstructed generated data (CVAE; σ'=0.1; Noise free)\n",
    "σ_0_1_Time_all_mu_nor_Re_er = sess.run(Re_X, feed_dict={X: σ_0_1_Time_all_mu_nor}) \n",
    "# Mean square value of each generated residual\n",
    "σ_0_1_Time_all_mu_nor_Re_er_list = np.mean((σ_0_1_Time_all_mu_nor - σ_0_1_Time_all_mu_nor_Re_er)**2, axis=1)\n",
    "\n",
    "#======(2) Caculate the fractions according to specific reconstruction error====\n",
    "\n",
    "# Training set \n",
    "Train_load_Re_er_list_stored = np.sort(Train_load_Re_er_list)\n",
    "Train_load_Re_er_list_fraction = np.array(range(len(Train_load_Re_er_list)))/float(len(Train_load_Re_er_list))\n",
    "\n",
    "# Testing set \n",
    "Test_load_Re_er_list_stored = np.sort(Test_load_Re_er_list)\n",
    "Test_load_Re_er_list_fraction = np.array(range(len(Test_load_Re_er_list)))/float(len(Test_load_Re_er_list))\n",
    "\n",
    "# Noisy generation set （CVAE; auto σ'; noisy）\n",
    "σ_auto_Time_all_noisy_nor_β_1_Re_er_list_stored = np.sort(σ_auto_Time_all_noisy_nor_β_1_Re_er_list)\n",
    "σ_auto_Time_all_noisy_nor_β_1_Re_er_list_fraction = np.array(range(len(σ_auto_Time_all_noisy_nor_β_1_Re_er_list)))/float(len(σ_auto_Time_all_noisy_nor_β_1_Re_er_list))\n",
    "    \n",
    "# Noisy generation set (CVAE; σ'=0.1; noisy)\n",
    "σ_0_1_Time_all_noisy_nor_Re_er_list_stored = np.sort(σ_0_1_Time_all_noisy_nor_Re_er_list)\n",
    "σ_0_1_Time_all_noisy_nor_Re_er_list_fraction = np.array(range(len(σ_0_1_Time_all_noisy_nor_Re_er_list)))/float(len(σ_0_1_Time_all_noisy_nor_Re_er_list))\n",
    "\n",
    "# Noise free generation set (CVAE; auto σ'; Noise free)\n",
    "σ_auto_Time_all_mu_nor_β_1_Re_er_list_stored = np.sort(σ_auto_Time_all_mu_nor_β_1_Re_er_list)\n",
    "σ_auto_Time_all_mu_nor_β_1_Re_er_list_fraction = np.array(range(len(σ_auto_Time_all_mu_nor_β_1_Re_er_list)))/float(len(σ_auto_Time_all_mu_nor_β_1_Re_er_list))\n",
    "\n",
    "# Noise free generation set (CVAE; σ'=0.1; Noise free)\n",
    "σ_0_1_Time_all_mu_nor_Re_er_list_stored = np.sort(σ_0_1_Time_all_mu_nor_Re_er_list)\n",
    "σ_0_1_Time_all_mu_nor_Re_er_list_fraction = np.array(range(len(σ_0_1_Time_all_mu_nor_Re_er_list)))/float(len(σ_0_1_Time_all_mu_nor_Re_er_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08323acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------Set figure--------------------\n",
    "# figure size\n",
    "plt.figure(figsize=(8,8))\n",
    "# Set dpi\n",
    "plt.rcParams['savefig.dpi'] = 600\n",
    "plt.rcParams['figure.dpi'] = 600\n",
    "# set frame\n",
    "bwith = 1.5\n",
    "TK = plt.gca() \n",
    "TK.spines['bottom'].set_linewidth(bwith)\n",
    "TK.spines['left'].set_linewidth(bwith)\n",
    "TK.spines['top'].set_linewidth(bwith)\n",
    "TK.spines['right'].set_linewidth(bwith)\n",
    "\n",
    "#------------------font--------------------------\n",
    "font = {'family' : 'Times New Roman',\n",
    "'weight' : 'normal','size': 24}\n",
    "\n",
    "#------------------set tick----------------------\n",
    "ax = plt.gca()\n",
    "ax.tick_params(direction='in', length=3, width=1)\n",
    "\n",
    "#------------------ tick labelel ----------------\n",
    "ax = plt.gca()\n",
    "ax.ticklabel_format(style='sci', scilimits=(-1,2), axis='y')\n",
    "ax.ticklabel_format(style='sci', scilimits=(-1,2), axis='x')\n",
    "tick_lable_size=20\n",
    "ax.set_xscale('log')\n",
    "\n",
    "#----------------- set gird width --------------\n",
    "gridwidth=1\n",
    "\n",
    "#---------------- set legend size --------------\n",
    "legend_size=24\n",
    "\n",
    "#----------------- set line width ---------------\n",
    "line_width=2\n",
    "\n",
    "\n",
    "# Training data\n",
    "plt.plot(Train_load_Re_er_list_stored, Train_load_Re_er_list_fraction, color='b',linestyle='-',\n",
    "         lw=line_width, label='Training data')\n",
    "\n",
    "# Test data\n",
    "#plt.plot(Test_load_Re_er_list_stored, Test_load_Re_er_list_fraction, color='b',linestyle='--',\n",
    "#         lw=line_width, label='Test data')\n",
    "\n",
    "# Noisy Generation set （CVAE auto noisy）\n",
    "plt.plot(σ_auto_Time_all_noisy_nor_β_1_Re_er_list_stored, σ_auto_Time_all_noisy_nor_β_1_Re_er_list_fraction, color='k',linestyle='-',\n",
    "         lw=line_width, label='CVAE (Auto σ, Noisy)')\n",
    "\n",
    "# Averaged Generation set (CVAE set noisy)\n",
    "plt.plot(σ_0_1_Time_all_noisy_nor_Re_er_list_stored, σ_0_1_Time_all_noisy_nor_Re_er_list_fraction, color='r',linestyle='-',\n",
    "         lw=line_width, label='CVAE (σ=0.1, Noisy)')\n",
    "\n",
    "# Noisy Generation set （CVAE auto mu）\n",
    "plt.plot(σ_auto_Time_all_mu_nor_β_1_Re_er_list_stored, σ_auto_Time_all_mu_nor_β_1_Re_er_list_fraction, color='k',linestyle=':',\n",
    "         lw=line_width, label='CVAE (Auto σ, Noise free)')\n",
    "\n",
    "\n",
    "plt.plot(σ_0_1_Time_all_mu_nor_Re_er_list_stored, σ_0_1_Time_all_mu_nor_Re_er_list_fraction, color='r',linestyle=':',\n",
    "         lw=line_width, label='CVAE (σ=0.1, Noise free)')\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "#---------------------------------------------------------------------------------\n",
    "#---------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ticks and grids\n",
    "plt.xticks(fontproperties = 'Times New Roman',size=tick_lable_size, weight = 'normal')\n",
    "plt.yticks(fontproperties = 'Times New Roman',fontsize=tick_lable_size, weight = 'normal')\n",
    "plt.grid(color='lightgrey', linestyle='-', linewidth=gridwidth, axis='y')\n",
    "plt.grid(color='lightgrey', linestyle='-', linewidth=gridwidth, axis='x')\n",
    "\n",
    "# Notations\n",
    "plt.xlabel('Reconstruction errors (a.u.)',font)\n",
    "plt.ylabel('Cumulative distribution',font)\n",
    "plt.legend(loc='best',prop={'family' : 'Times New Roman','weight' : 'normal','size': legend_size})\n",
    "\n",
    "plt.text(0.00104, -0.23,'(e)',font)\n",
    "plt.xlim(0.00001,0.155)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
