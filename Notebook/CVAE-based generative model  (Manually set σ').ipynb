{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this code\n",
    "\n",
    "- Code author: Chenguang Wang     \n",
    "- Email: c.wang-8@tudelft.nl; samwangchenguang@gmail.com\n",
    "- Affiliation: Delft University of Technology\n",
    "- Project name: Generating multivariate load states using a (conditional) variational autoencoder\n",
    "- Motivation: This is a project for PSCC2022 – Power Systems Computation Conference: [Homepage of the conference](https://pscc2022.pt/)\n",
    "- Aim of this code: Generating 32-dimensional load states using a conditional variational autoencoder with fixed output noise parameters (Manually set σ')\n",
    "- A preprint is available, and you can check this paper for more details  [Link of the paper](https://arxiv.org/abs/2110.11435)\n",
    "    - Paper authors: Chenguang Wang, Ensieh Sharifnia, Zhi Gao, Simon H. Tindemans, Peter Palensky\n",
    "    - Accepted for publication at PSCC 2022 and a special issue of EPSR\n",
    "    - If you use (parts of) this code, please cite the preprint or published paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skisMfE-8smQ"
   },
   "source": [
    "# Preparation (Library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1629117294802,
     "user": {
      "displayName": "王晨光",
      "photoUrl": "",
      "userId": "13859634542690233274"
     },
     "user_tz": -120
    },
    "id": "QLbQ8NEBdECl",
    "outputId": "f2554866-6cc1-4bf3-e299-ae6e70782690"
   },
   "outputs": [],
   "source": [
    "# ==================== Preparation (library) =======================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvAbNvBb81FJ"
   },
   "source": [
    "# Preparation (Data)\n",
    "- Input: Load data for 32 European countries between 2013 and 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ojUp-ix4K9R8"
   },
   "outputs": [],
   "source": [
    "# ============================ Get all data ====================================\n",
    "\n",
    "Data = pd.read_csv(\"../Data/13-17_32_conditioned.csv\") # National_load_data\n",
    "Train = pd.read_csv(\"../Data/13-17_32_Train.csv\", index_col=0) # Training data\n",
    "Test = pd.read_csv(\"../Data/13-17_32_Test.csv\", index_col=0) # Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0llPZY-lLyND"
   },
   "outputs": [],
   "source": [
    "# ================= Separate load data and time label ==========================\n",
    "\n",
    "# Get column location\n",
    "Austria=Data.columns.get_loc('AT_load_actual_entsoe_power_statistics')\n",
    "Slovakia=Data.columns.get_loc('SK_load_actual_entsoe_power_statistics')\n",
    "hour_sin=Data.columns.get_loc('hour_sin')\n",
    "hour_cos=Data.columns.get_loc('hour_cos')\n",
    "\n",
    "# Get only load data from training data set\n",
    "Train_load=Train.iloc[:,Austria:(Slovakia+1)]\n",
    "Train_load.index=range(len(Train_load)) # index rearrange\n",
    "\n",
    "# Get only load data from test data set\n",
    "Test_load=Test.iloc[:,Austria:(Slovakia+1)]\n",
    "Test_load.index=range(len(Test_load)) # index rearrange\n",
    "\n",
    "# Get only time label from training data set\n",
    "Train_label=Train.iloc[:,hour_sin:(hour_cos+1)]\n",
    "Train_label.index=range(len(Train_label)) # index rearrange\n",
    "\n",
    "# Get only time label from test data set\n",
    "Test_label=Test.iloc[:,hour_sin:(hour_cos+1)]\n",
    "Test_label.index=range(len(Test_label)) # index rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhgH902CLkbu"
   },
   "outputs": [],
   "source": [
    "# ============= Data processing for all training and testing data  =============\n",
    "#============================= Traing data processing ==========================\n",
    "\n",
    "# To get \"normalized\" training data\n",
    "Tr = Train_load\n",
    "max_Tr = np.max(Tr, axis = 0) # reserve the maximum value of basic training data\n",
    "min_Tr = np.min(Tr, axis = 0) # reserve the minimum value of basic training data\n",
    "Tr_nor = (Tr-min_Tr)/(max_Tr-min_Tr) # normalized training data\n",
    "\n",
    "mean_Tr = np.mean(Tr, axis = 0) # Mean Value\n",
    "\n",
    "min_Tr_astype = min_Tr.values.astype(np.float32) # Training \n",
    "max_Tr_astype = max_Tr.values.astype(np.float32) # Testing \n",
    "mean_Tr_astyped = mean_Tr.values.astype(np.float32) # Mean\n",
    "\n",
    "# To get \"normalized\" Testing data\n",
    "Te = Test_load\n",
    "Te_nor = (Te-min_Tr)/(max_Tr-min_Tr)\n",
    "\n",
    "# \"Date format\" conversion for all data\n",
    "TR_load = Tr_nor.values.astype(np.float32) # Training Data                                                                                                                \n",
    "TE_load  = Te_nor.values.astype(np.float32) # Testing Data\n",
    "TR_label = Train_label.values.astype(np.float32) # Training label\n",
    "TE_label = Test_label.values.astype(np.float32) # Testing label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lls2MNGGTLlA"
   },
   "outputs": [],
   "source": [
    "# ====== Data processing for specific hour of training and testing data  =======\n",
    "# ================= Split load data at specific time ===========================\n",
    "\n",
    "# Define empty list\n",
    "Index=[None] * 24 # 24 hours\n",
    "Train_load_at=[None] * 24\n",
    "Train_load_volume_at=[None] * 24\n",
    "Test_load_at=[None] * 24\n",
    "Test_load_volume_at=[None] * 24\n",
    "# Training load data at specific time\n",
    "for i in range(len(Index)):\n",
    "  Index[i] = Train[Train['hour']==i].index.tolist() # Get the index of specific time\n",
    "  Train_load_at[i]=Train_load.iloc[Index[i]] # Get load data of specific time\n",
    "  Train_load_at[i].index=range(len(Train_load_at[i])) # Rearrange the load index of data at specific time\n",
    "  Train_load_volume_at[i]=len(Train_load_at[i]) # Get volume of load data at specific time\n",
    "# Testing load data at specific time\n",
    "for i in range(len(Index)):\n",
    "  Index[i] = Test[Test['hour']==i].index.tolist() # Get the index of specific time\n",
    "  Test_load_at[i]=Test_load.iloc[Index[i]] # Get load data of specific time\n",
    "  Test_load_at[i].index=range(len(Test_load_at[i])) # Rearrange the load index of data at specific time\n",
    "  Test_load_volume_at[i]=len(Test_load_at[i]) # Get volume of load data at specific time\n",
    "\n",
    "# ================= Split label at specific time ===========================\n",
    "\n",
    "# Define empty list\n",
    "Index=[None] * 24 # 24 hours\n",
    "Train_label_at=[None] * 24\n",
    "Train_label_volume_at=[None] * 24\n",
    "Test_label_at=[None] * 24\n",
    "Test_label_volume_at=[None] * 24\n",
    "# Training label at specific time\n",
    "for i in range(len(Index)):\n",
    "  Index[i] = Train[Train['hour']==i].index.tolist() # Get the index of specific time\n",
    "  Train_label_at[i]=Train_label.iloc[Index[i]] # Get label of specific time\n",
    "  Train_label_at[i].index=range(len(Train_label_at[i])) # Rearrange the label index of data at specific time\n",
    "  Train_label_volume_at[i]=len(Train_label_at[i]) # Get volume of data at specific time\n",
    "# Testing label at specific time\n",
    "for i in range(len(Index)):\n",
    "  Index[i] = Test[Test['hour']==i].index.tolist() # Get the index of specific time\n",
    "  Test_label_at[i]=Test_label.iloc[Index[i]] # Get label of specific time\n",
    "  Test_label_at[i].index=range(len(Test_label_at[i])) # Rearrange the label index of data at specific time\n",
    "  Test_label_volume_at[i]=len(Test_label_at[i]) # Get volume of data at specific time\n",
    "\n",
    "# To get \"normalized\" training load data at specific hour\n",
    "Tr_nor_2 = (Train_load_at[2]-min_Tr)/(max_Tr-min_Tr) \n",
    "Tr_nor_10 = (Train_load_at[10]-min_Tr)/(max_Tr-min_Tr)\n",
    "Tr_nor_21 = (Train_load_at[21]-min_Tr)/(max_Tr-min_Tr)\n",
    "\n",
    "# Date \"format conversion\" for hour of training data\n",
    "# Training load data at 2:00 \n",
    "TR_load_2 = Tr_nor_2.values.astype(np.float32) \n",
    "# Training load data at 10:00\n",
    "TR_load_10 = Tr_nor_10.values.astype(np.float32) \n",
    "# Training load data at 21:00\n",
    "TR_load_21 = Tr_nor_21.values.astype(np.float32) \n",
    "\n",
    "# Training Label at 2:00\n",
    "TR_label_2 = Train_label_at[2].values.astype(np.float32)\n",
    "# Training Label at 10:00\n",
    "TR_label_10 = Train_label_at[10].values.astype(np.float32)\n",
    "# Training Label at 21:00\n",
    "TR_label_21 = Train_label_at[21].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rh10xpPQuROZ"
   },
   "source": [
    "# Preparation (Neuro Network Dessign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 327,
     "status": "ok",
     "timestamp": 1629117296977,
     "user": {
      "displayName": "王晨光",
      "photoUrl": "",
      "userId": "13859634542690233274"
     },
     "user_tz": -120
    },
    "id": "09e5boHVIgGh",
    "outputId": "a180476b-0543-4254-d8d7-f796bceb04b2"
   },
   "outputs": [],
   "source": [
    "# ================== Preparation (Neuro network dessign) =======================\n",
    "\n",
    "# ========================== Initialization fuction ============================\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "\n",
    "# ===================== Data dimension calcuation ==============================\n",
    "\n",
    "# Dimension of the input\n",
    "X_dim_all=np.shape(Train_load)\n",
    "X_dim = X_dim_all[1]\n",
    "# Dimension of the time label\n",
    "y_dim_all =np.shape(TR_label) \n",
    "y_dim = y_dim_all[1]\n",
    "# Dimension of the hidden layer\n",
    "h_dim_1 = 24\n",
    "# Dimension of the hidden layer\n",
    "h_dim_2 = 16\n",
    "# Dimension of the latent space\n",
    "z_dim = 8\n",
    "\n",
    "# ============================= Print the dimension ============================\n",
    "\n",
    "print ('The dimension of the input data is: ', X_dim)\n",
    "print ('The dimension of the hidden layer 1 is: ', h_dim_1)\n",
    "print ('The dimension of the hidden layer 2 is: ', h_dim_2)\n",
    "print ('The dimension of the latent space data is: ', z_dim)\n",
    "\n",
    "# =============================== Q(z|X,c) =====================================\n",
    "\n",
    "# Data\n",
    "X = tf.placeholder(tf.float32, shape=[None, X_dim])\n",
    "c = tf.placeholder(tf.float32, shape=[None, y_dim])\n",
    "z = tf.placeholder(tf.float32, shape=[None, z_dim])\n",
    "\n",
    "# Layer definitions\n",
    "Q_W1 = tf.Variable(xavier_init([X_dim+y_dim, h_dim_1]))\n",
    "Q_b1 = tf.Variable(tf.zeros(shape=[h_dim_1]))\n",
    "\n",
    "Q_W2 = tf.Variable(xavier_init([h_dim_1, h_dim_2]))\n",
    "Q_b2 = tf.Variable(tf.zeros(shape=[h_dim_2]))\n",
    "\n",
    "Q_W3_mu = tf.Variable(xavier_init([h_dim_2, z_dim]))\n",
    "Q_b3_mu = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "Q_W3_sigma = tf.Variable(xavier_init([h_dim_2, z_dim]))\n",
    "Q_b3_sigma = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "# Network\n",
    "def Q(X, c): \n",
    "    inputs = tf.concat(axis=1, values=[X, c]) \n",
    "    h1 = tf.nn.relu((tf.matmul(inputs, Q_W1) + Q_b1))\n",
    "    h2 = tf.nn.relu((tf.matmul(h1, Q_W2) + Q_b2))\n",
    "    z_mu = tf.matmul(h2, Q_W3_mu) + Q_b3_mu\n",
    "    z_logvar = tf.matmul(h2, Q_W3_sigma) + Q_b3_sigma\n",
    "    return z_mu, z_logvar\n",
    "\n",
    "# ================Sampling a z from N~(z_mu|tf.exp(z_logvar))===================\n",
    "\n",
    "def sample_z(mu, log_var):\n",
    "    eps = tf.random_normal(shape=tf.shape(mu))\n",
    "    return mu + tf.exp(log_var / 2) * eps\n",
    "\n",
    "# =============================== P(X|z,c) =======================================\n",
    "\n",
    "# Layer definitions\n",
    "P_W1 = tf.Variable(xavier_init([z_dim + y_dim , h_dim_2]))\n",
    "P_b1 = tf.Variable(tf.zeros(shape=[h_dim_2]))\n",
    "\n",
    "P_W2 = tf.Variable(xavier_init([h_dim_2 , h_dim_1]))  \n",
    "P_b2 = tf.Variable(tf.zeros(shape=[h_dim_1]))\n",
    "\n",
    "P_W3_mu = tf.Variable(xavier_init([h_dim_1, X_dim]))\n",
    "P_b3_mu = tf.Variable(tf.zeros(shape=[X_dim]))\n",
    "\n",
    "P_W3_logvar = tf.Variable(xavier_init([h_dim_1, X_dim]))\n",
    "P_b3_logvar = tf.Variable(tf.zeros(shape=[X_dim]))\n",
    "\n",
    "# Define the manually-set σ as a hyperparameter\n",
    "σ_manually_set=0.1 # manually-set σ\n",
    "\n",
    "# Network\n",
    "def P(z,c):\n",
    "    inputs = tf.concat(axis=1, values=[z, c]) \n",
    "    h1 = tf.nn.relu((tf.matmul(inputs, P_W1) + P_b1))\n",
    "    h2 = tf.nn.relu((tf.matmul(h1, P_W2) + P_b2))\n",
    "    Gen_mu = tf.matmul(h2, P_W3_mu) + P_b3_mu # Gen_mu : μ_x(z) \n",
    "    eps_p = tf.random_normal(shape=tf.shape(Gen_mu)) # epsilon\n",
    "    Gen_σ=tf.ones(shape=tf.shape(Gen_mu))*σ_manually_set # Gen_σ:σ (manually-set σ of all dimensions)\n",
    "    Gen_noisy = Gen_mu +eps_p*Gen_σ # Gen_noisy = μ_x(z) + eps_p* σ \n",
    "    return Gen_mu, Gen_noisy    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxhv_Dqx9C2r"
   },
   "source": [
    "# Training (CVAE; Manually set σ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 430871,
     "status": "ok",
     "timestamp": 1629117727821,
     "user": {
      "displayName": "王晨光",
      "photoUrl": "",
      "userId": "13859634542690233274"
     },
     "user_tz": -120
    },
    "id": "K6xIvQgFgtxO",
    "outputId": "f47df34f-18ba-4b2b-8d05-82ae0e73536a"
   },
   "outputs": [],
   "source": [
    "# ================================ Training ====================================\n",
    "\n",
    "# Training condition: lr (learning rate)=0.0001; batch_size (mb_size) =64;\n",
    "#                     iteration=200,000; σ' Manually set\n",
    "\n",
    "#Traning setting \n",
    "mb_size = 64 # Batch size\n",
    "lr = 0.0001 #learning rate\n",
    "β = 1 # weight ratio of kl_loss and recon_loss \n",
    "iteration = 200000 # define the iteration steps\n",
    "\n",
    "# Training functions\n",
    "z_mu, z_logvar = Q(X, c) # z follows N~(z_μ,z_log(σ^2)) \n",
    "z_sample = sample_z(z_mu, z_logvar) # Sampling z_sample from N~(z_μ, tf.exp(z_log(σ^2)))\n",
    "Gen_mu_train, Gen_noisy_train = P(z_sample, c) # generate (hat) x_μ, x_log(σ^2), x given z_sample,c\n",
    "\n",
    "# Generation functions\n",
    "Gen_mu_sample, Gen_noisy_sample = P(z, c) # Sampling (tilde) x_μ, x_log(σ^2), x given z follows N~(0|1),c \n",
    "\n",
    "# Training loss when σ' is manually set\n",
    "recon_loss = 0.5 *tf.reduce_sum((Gen_mu_train - X)**2/(σ_manually_set**2)+tf.math.log(σ_manually_set**2), 1) # recon_error\n",
    "\n",
    "kl_loss  = β*0.5 * tf.reduce_sum(tf.exp(z_logvar) - 1. - z_logvar + z_mu**2, 1) # KL_error\n",
    "\n",
    "cvae_loss = tf.reduce_mean(recon_loss + kl_loss)+X_dim/2*tf.log(2*(math.pi)) # CVAE loss\n",
    "\n",
    "# Monitor what the exact number \n",
    "recon_loss_mean = tf.reduce_mean(recon_loss)+X_dim/2*tf.log(2*(math.pi))\n",
    "\n",
    "kl_loss_mean = tf.reduce_mean(kl_loss)\n",
    "\n",
    "# Solver\n",
    "solver = tf.train.AdamOptimizer(lr).minimize(cvae_loss)\n",
    "\n",
    "# Seesion\n",
    "sess = tf.Session() # Session definition\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for it in range(iteration): # Training process\n",
    "    batch_index = np.random.choice(TR_load.shape[0], size=mb_size, replace=False)\n",
    "    _, loss, KL_error, Recon_error = sess.run([solver, cvae_loss, kl_loss_mean, recon_loss_mean], \n",
    "                                              feed_dict={X: TR_load[batch_index], c:TR_label[batch_index]})                                                                                                                    \n",
    "    if it % 1000 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('Loss: {:.4}'. format(loss))\n",
    "        print('KL_error: {}'. format(KL_error))\n",
    "        print('Recon_error: {}'. format(Recon_error))\n",
    "        print()\n",
    "print(\"Training Finished!\") \n",
    "\n",
    "#Save the reslut    \n",
    "saver = tf.train.Saver() \n",
    "saver.save(sess, './filename.chkp') \n",
    "\n",
    "# Session Recover\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()                                                     \n",
    "saver.restore(sess, 'filename.chkp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5Z_dFCR9Ijl"
   },
   "source": [
    "# Load data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIGD5X1rmF3F"
   },
   "outputs": [],
   "source": [
    "# ==========================Generate load data==================================\n",
    "\n",
    "# =======================Function to recover data===============================\n",
    "\n",
    "def Gen_recover_function(Gen_to_be_recovered): \n",
    "    Gen_recovered=Gen_to_be_recovered*(max_Tr_astype-min_Tr_astype)+min_Tr_astype\n",
    "    return Gen_recovered\n",
    "\n",
    "# =========================Geneter load data from 0-23 o'clock ==================\n",
    "Gen_mu_at=[None]* 24\n",
    "Gen_mu_recovered_at=[None] * 24\n",
    "\n",
    "Gen_noisy_at=[None]* 24\n",
    "Gen_noisy_recovered_at=[None] * 24\n",
    "\n",
    "for Time in range(len(Gen_mu_at)):   \n",
    "    # Get labels\n",
    "    GEN_label = np.zeros(shape=[Train_load_volume_at[Time], y_dim])\n",
    "    GEN_label[:,0]=np.sin(Time/12*math.pi).round(2)\n",
    "    GEN_label[:,1]=np.cos(Time/12*math.pi).round(2)\n",
    "    # Generate data\n",
    "    Gen_mu_new, Gen_noisy_new = sess.run([Gen_mu_sample, Gen_noisy_sample], \n",
    "                                             feed_dict={z: np.random.randn(Train_load_volume_at[Time],z_dim), c: GEN_label})\n",
    "        \n",
    "    # Generate data mu (rescaled）\n",
    "    Gen_mu_at[Time] = Gen_mu_new # Gen_mu_new\n",
    "    Gen_mu_recovered_at[Time] =  Gen_recover_function(Gen_mu_new) # Gen_mu_new (rescaled)\n",
    "    \n",
    "    # Generate data noisy (rescaled）\n",
    "    Gen_noisy_at[Time] = Gen_noisy_new # Gen_noisy_new \n",
    "    Gen_noisy_recovered_at[Time] = Gen_recover_function(Gen_noisy_new) # Gen_noisy_new (rescaled)\n",
    "\n",
    "    if Time == 0:\n",
    "      Gen_mu_all = Gen_mu_at[Time]    # All generated data mu\n",
    "      Gen_mu_recovered_all = Gen_mu_recovered_at[Time]   # All generated data mu (rescaled)\n",
    "      Gen_noisy_all = Gen_noisy_at[Time] # All generated data noisy\n",
    "      Gen_noisy_recovered_all = Gen_noisy_recovered_at[Time]   # All generated data noisy (rescaled)\n",
    "    else:\n",
    "      Gen_mu_all = np.concatenate((Gen_mu_all, Gen_mu_at[Time]),axis=0)    \n",
    "      Gen_mu_recovered_all = np.concatenate((Gen_mu_recovered_all, Gen_mu_recovered_at[Time]),axis=0)  \n",
    "      Gen_noisy_all = np.concatenate((Gen_noisy_all, Gen_noisy_at[Time]),axis=0)    \n",
    "      Gen_noisy_recovered_all = np.concatenate((Gen_noisy_recovered_all, Gen_noisy_recovered_at[Time]),axis=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation for the file names\n",
    "- filename with \"σ'_auto\": output noise parameter is co-optimised during training (Auto σ')\n",
    "- filename with \"σ'_\"+ a \"number\": output noise parameter is  fixed (Manually set σ')\n",
    "\n",
    "- filename with \"mu\": generated data without noise (noise free)\n",
    "- filename with \"noisy\": generated data with noise (noisy)\n",
    "\n",
    "- filename with \"Time_all\": generated data for 24 hours\n",
    "- filename with \"Time_\"+ a \"number\": generated data for a specific hour of the day\n",
    "\n",
    "- filename with \"nor\": generated data that are not rescaled to the original scale of historical data\n",
    "- filename without \"nor\": generated data that are rescaled to the original scale of historical data\n",
    "\n",
    "- filename with a β number: data generated with that specific β number\n",
    "- filename without a β number: the number of β is 1 by default\n",
    "\n",
    "- filename with \"VAE\" specified: it is a VAE model\n",
    "- filename without \"VAE\" specified: it is a CVAE model by default\n",
    "\n",
    "- filename with \"35\" in the end: a model for 35-dimensional input\n",
    "- filename without \"35\" in the end: a model for 32-dimensional input by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OErVzfvRgwRu"
   },
   "outputs": [],
   "source": [
    "# Save data\n",
    "pd.DataFrame(Gen_mu_recovered_at[2]).to_csv(\"σ'_{:.2f}_Time_2_mu.csv\".format(σ_manually_set))\n",
    "pd.DataFrame(Gen_mu_recovered_at[10]).to_csv(\"σ'_{:.2f}_Time_10_mu.csv\".format(σ_manually_set))\n",
    "pd.DataFrame(Gen_mu_recovered_at[21]).to_csv(\"σ'_{:.2f}_Time_21_mu.csv\".format(σ_manually_set))\n",
    "pd.DataFrame(Gen_noisy_recovered_at[2]).to_csv(\"σ'_{:.2f}_Time_2_noisy.csv\".format(σ_manually_set))\n",
    "pd.DataFrame(Gen_noisy_recovered_at[10]).to_csv(\"σ'_{:.2f}_Time_10_noisy.csv\".format(σ_manually_set))\n",
    "pd.DataFrame(Gen_noisy_recovered_at[21]).to_csv(\"σ'_{:.2f}_Time_21_noisy.csv\".format(σ_manually_set))\n",
    "pd.DataFrame(Gen_mu_recovered_all).to_csv(\"σ'_{:.2f}_Time_all_mu.csv\".format(σ_manually_set))\n",
    "pd.DataFrame(Gen_noisy_recovered_all).to_csv(\"σ'_{:.2f}_Time_all_noisy.csv\".format(σ_manually_set))\n",
    "pd.DataFrame(Gen_mu_all).to_csv(\"σ'_{:.2f}_Time_all_mu_nor.csv\".format(σ_manually_set))\n",
    "pd.DataFrame(Gen_noisy_all).to_csv(\"σ'_{:.2f}_Time_all_noisy_nor.csv\".format(σ_manually_set))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPCATpZHArjlpsT3FEN6xrx",
   "collapsed_sections": [
    "skisMfE-8smQ",
    "yvAbNvBb81FJ",
    "r513Ha890qDv",
    "-xT6IRwfOJOI",
    "scr8FbkxGMBK",
    "KixHk4ecew5P"
   ],
   "name": "13-17_32_CVAE.ipynb (Manually set sigma)",
   "provenance": [
    {
     "file_id": "1ZU50rlbIt5HHxBJuixK5D37Fz0M4WkH3",
     "timestamp": 1629113930609
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
