{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this code\n",
    "\n",
    "- Author: Chenguang Wang   \n",
    "- Email: samwangchenguang@gmail.com\n",
    "- Project name: Generating multivariate load states using a (conditional) variational autoencoder\n",
    "- Motivation: This is a project for PSCC2022 – Power Systems Computation Conference: [Homepage of the conference](https://pscc2022.pt/)\n",
    "- Aim of this code: Generating 32-dimensional load states using variational autoencoder with fixed output noise parameters (Manually set σ')\n",
    "- Reference Paper: You can check our paper for more details: [Link of the paper](https://arxiv.org/abs/2110.11435)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skisMfE-8smQ"
   },
   "source": [
    "# Preparation (Library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44745,
     "status": "ok",
     "timestamp": 1630847686178,
     "user": {
      "displayName": "王晨光",
      "photoUrl": "",
      "userId": "13859634542690233274"
     },
     "user_tz": -120
    },
    "id": "QLbQ8NEBdECl",
    "outputId": "bb01f0b1-2369-45dc-ad3d-695aa75c2995",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ==================== Preparation (library) =======================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvAbNvBb81FJ"
   },
   "source": [
    "# Preparation (Data)\n",
    "- Input: Load data for 32 European countries between 2013 and 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 437,
     "status": "ok",
     "timestamp": 1630847998454,
     "user": {
      "displayName": "王晨光",
      "photoUrl": "",
      "userId": "13859634542690233274"
     },
     "user_tz": -120
    },
    "id": "ojUp-ix4K9R8"
   },
   "outputs": [],
   "source": [
    "# ============================ Get all data ====================================\n",
    "\n",
    "Data = pd.read_csv(\"../Data/13-17_32_conditioned.csv\") # National_load_data\n",
    "Train = pd.read_csv(\"../Data/13-17_32_Train.csv\", index_col=0) # Training data\n",
    "Test = pd.read_csv(\"../Data/13-17_32_Test.csv\", index_col=0) # Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 823,
     "status": "ok",
     "timestamp": 1630848747916,
     "user": {
      "displayName": "王晨光",
      "photoUrl": "",
      "userId": "13859634542690233274"
     },
     "user_tz": -120
    },
    "id": "UH8F4U5iLeuS"
   },
   "outputs": [],
   "source": [
    "# ================= Separate load data and time label ==========================\n",
    "\n",
    "# Get column location\n",
    "Austria=Data.columns.get_loc('AT_load_actual_entsoe_power_statistics')\n",
    "Slovakia=Data.columns.get_loc('SK_load_actual_entsoe_power_statistics')\n",
    "hour_sin=Data.columns.get_loc('hour_sin')\n",
    "hour_cos=Data.columns.get_loc('hour_cos')\n",
    "\n",
    "# Get only load data from training data set\n",
    "Train_load=Train.iloc[:,Austria:(Slovakia+1)]\n",
    "Train_load.index=range(len(Train_load)) # index rearrange\n",
    "\n",
    "# Get only load data from test data set\n",
    "Test_load=Test.iloc[:,Austria:(Slovakia+1)]\n",
    "Test_load.index=range(len(Test_load)) # index rearrange\n",
    "\n",
    "# Get only time label from training data set\n",
    "Train_label=Train.iloc[:,hour_sin:(hour_cos+1)]\n",
    "Train_label.index=range(len(Train_label)) # index rearrange\n",
    "\n",
    "# Get only time label from test data set\n",
    "Test_label=Test.iloc[:,hour_sin:(hour_cos+1)]\n",
    "Test_label.index=range(len(Test_label)) # index rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1630848653533,
     "user": {
      "displayName": "王晨光",
      "photoUrl": "",
      "userId": "13859634542690233274"
     },
     "user_tz": -120
    },
    "id": "ciUnV9i8miHk"
   },
   "outputs": [],
   "source": [
    "# ============= Dara processing for all training and testing data  =============\n",
    "#============================= Traing data processing ==========================\n",
    "\n",
    "# To get \"normalized\" training data\n",
    "Tr = Train_load\n",
    "max_Tr = np.max(Tr, axis = 0) # reserve the maximum value of basic training data\n",
    "min_Tr = np.min(Tr, axis = 0) # reserve the minimum value of basic training data\n",
    "Tr_nor = (Tr-min_Tr)/(max_Tr-min_Tr) # normalized training data\n",
    "\n",
    "mean_Tr = np.mean(Tr, axis = 0) # Mean Value\n",
    "\n",
    "min_Tr_astype = min_Tr.values.astype(np.float32) # Training \n",
    "max_Tr_astype = max_Tr.values.astype(np.float32) # Testing \n",
    "mean_Tr_astyped = mean_Tr.values.astype(np.float32) # Mean\n",
    "\n",
    "# To get \"normalized\" Testing data\n",
    "Te = Test_load\n",
    "Te_nor = (Te-min_Tr)/(max_Tr-min_Tr)\n",
    "\n",
    "# \"Date format\" conversion for all data\n",
    "TR_load = Tr_nor.values.astype(np.float32) # Training Data                                                                                                                \n",
    "TE_load  = Te_nor.values.astype(np.float32) # Testing Data\n",
    "TR_label = Train_label.values.astype(np.float32) # Training label\n",
    "TE_label = Test_label.values.astype(np.float32) # Testing label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rh10xpPQuROZ"
   },
   "source": [
    "# Preparation (Neuro Network Dessign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 71,
     "status": "ok",
     "timestamp": 1629189108624,
     "user": {
      "displayName": "王晨光",
      "photoUrl": "",
      "userId": "13859634542690233274"
     },
     "user_tz": -120
    },
    "id": "09e5boHVIgGh",
    "outputId": "538021f6-2e5b-44cd-e32d-032c4f6e470b"
   },
   "outputs": [],
   "source": [
    "# ================== Preparation (Neuro network dessign) =======================\n",
    "\n",
    "# ========================== Initialization fuction ============================\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "\n",
    "# ===================== Data dimension calcuation ==============================\n",
    "\n",
    "# Dimension of the input\n",
    "X_dim_all=np.shape(Train_load)\n",
    "X_dim = X_dim_all[1]\n",
    "# Dimension of the time label\n",
    "y_dim_all =np.shape(TR_label) \n",
    "y_dim = y_dim_all[1]\n",
    "# Dimension of the hidden layer\n",
    "h_dim_1 = 24\n",
    "# Dimension of the hidden layer\n",
    "h_dim_2 = 16\n",
    "# Dimension of the latent space\n",
    "z_dim = 8\n",
    "\n",
    "# ============================= Print the dimension ============================\n",
    "\n",
    "print ('The dimension of the input data is: ', X_dim)\n",
    "print ('The dimension of the hidden layer 1 is: ', h_dim_1)\n",
    "print ('The dimension of the hidden layer 2 is: ', h_dim_2)\n",
    "print ('The dimension of the latent space data is: ', z_dim)\n",
    "\n",
    "# =============================== Q(z|X) =====================================\n",
    "\n",
    "# Data\n",
    "X = tf.placeholder(tf.float32, shape=[None, X_dim])\n",
    "z = tf.placeholder(tf.float32, shape=[None, z_dim])\n",
    "\n",
    "# Layer definitions\n",
    "Q_W1 = tf.Variable(xavier_init([X_dim, h_dim_1]))\n",
    "Q_b1 = tf.Variable(tf.zeros(shape=[h_dim_1]))\n",
    "\n",
    "Q_W2 = tf.Variable(xavier_init([h_dim_1, h_dim_2]))\n",
    "Q_b2 = tf.Variable(tf.zeros(shape=[h_dim_2]))\n",
    "\n",
    "Q_W3_mu = tf.Variable(xavier_init([h_dim_2, z_dim]))\n",
    "Q_b3_mu = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "Q_W3_sigma = tf.Variable(xavier_init([h_dim_2, z_dim]))\n",
    "Q_b3_sigma = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "# Network\n",
    "def Q(X): \n",
    "    h1 = tf.nn.relu((tf.matmul(X, Q_W1) + Q_b1))\n",
    "    h2 = tf.nn.relu((tf.matmul(h1, Q_W2) + Q_b2))\n",
    "    z_mu = tf.matmul(h2, Q_W3_mu) + Q_b3_mu\n",
    "    z_logvar = tf.matmul(h2, Q_W3_sigma) + Q_b3_sigma\n",
    "    return z_mu, z_logvar\n",
    "\n",
    "# ================Sampling a z from N~(z_mu|tf.exp(z_logvar))===================\n",
    "\n",
    "def sample_z(mu, log_var):\n",
    "    eps = tf.random_normal(shape=tf.shape(mu))\n",
    "    return mu + tf.exp(log_var / 2) * eps\n",
    "\n",
    "# =============================== P(X|z) =======================================\n",
    "\n",
    "# Layer definitions\n",
    "P_W1 = tf.Variable(xavier_init([z_dim, h_dim_2]))\n",
    "P_b1 = tf.Variable(tf.zeros(shape=[h_dim_2]))\n",
    "\n",
    "P_W2 = tf.Variable(xavier_init([h_dim_2 , h_dim_1]))  \n",
    "P_b2 = tf.Variable(tf.zeros(shape=[h_dim_1]))\n",
    "\n",
    "P_W3_mu = tf.Variable(xavier_init([h_dim_1, X_dim]))\n",
    "P_b3_mu = tf.Variable(tf.zeros(shape=[X_dim]))\n",
    "\n",
    "P_W3_logvar = tf.Variable(xavier_init([h_dim_1, X_dim]))\n",
    "P_b3_logvar = tf.Variable(tf.zeros(shape=[X_dim]))\n",
    "\n",
    "# Define the manually-set σ as a hyperparameter\n",
    "σ_manually_set=0.1 # manually-set σ\n",
    "\n",
    "# Network\n",
    "def P(z):\n",
    "    h1 = tf.nn.relu((tf.matmul(z, P_W1) + P_b1))\n",
    "    h2 = tf.nn.relu((tf.matmul(h1, P_W2) + P_b2))\n",
    "    Gen_mu = tf.matmul(h2, P_W3_mu) + P_b3_mu # Gen_mu : μ_x(z) \n",
    "    eps_p = tf.random_normal(shape=tf.shape(Gen_mu)) # epsilon\n",
    "    Gen_σ=tf.ones(shape=tf.shape(Gen_mu))*σ_manually_set # Gen_σ:σ (manually-set σ of all dimensions)\n",
    "    Gen_noisy = Gen_mu +eps_p*Gen_σ # Gen_noisy = μ_x(z) + eps_p* σ \n",
    "    return Gen_mu, Gen_noisy    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxhv_Dqx9C2r"
   },
   "source": [
    "# Training (VAE; Manually set σ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 442306,
     "status": "ok",
     "timestamp": 1629189550864,
     "user": {
      "displayName": "王晨光",
      "photoUrl": "",
      "userId": "13859634542690233274"
     },
     "user_tz": -120
    },
    "id": "K6xIvQgFgtxO",
    "outputId": "615d558a-a41c-4a7a-860a-ecc993d74b82"
   },
   "outputs": [],
   "source": [
    "# ================================ Training ====================================\n",
    "\n",
    "# Training condition: lr (learning rate)=0.0001; batch_size (mb_size) =64;\n",
    "#                     iteration=200,000; σ' Manually set\n",
    "\n",
    "#Traning setting \n",
    "mb_size = 64 # Batch size\n",
    "lr = 0.0001 #learning rate\n",
    "β = 1 # weight ratio of kl_loss and recon_loss \n",
    "iteration = 200000 # define the epoch\n",
    "\n",
    "# Training functions\n",
    "z_mu, z_logvar = Q(X) # z follows N~(z_μ,z_log(σ^2)) \n",
    "z_sample = sample_z(z_mu, z_logvar) # Sampling z_sample from N~(z_μ, tf.exp(z_log(σ^2)))\n",
    "Gen_mu_train, Gen_noisy_train = P(z_sample) # generate (hat) x_μ, x_log(σ^2), x given z_sample\n",
    "\n",
    "# Generation functions\n",
    "Gen_mu_sample, Gen_noisy_sample = P(z) # Sampling (tilde) x_μ, x_log(σ^2), x given z follows N~(0|1)\n",
    "\n",
    "# Training loss when σ_x()_automatically generated\n",
    "recon_loss = 0.5 *tf.reduce_sum((Gen_mu_train - X)**2/(σ_manually_set**2)+tf.math.log(σ_manually_set**2), 1) # recon_error\n",
    "\n",
    "kl_loss  =   β*0.5 * tf.reduce_sum(tf.exp(z_logvar) - 1. - z_logvar + z_mu**2, 1) # KL_error\n",
    "\n",
    "vae_loss =   tf.reduce_mean(recon_loss + kl_loss)+X_dim/2*tf.log(2*(math.pi)) # VAE loss\n",
    "\n",
    "# Monitor what the exact number \n",
    "recon_loss_mean = tf.reduce_mean(recon_loss)+X_dim/2*tf.log(2*(math.pi))\n",
    "\n",
    "kl_loss_mean = tf.reduce_mean(kl_loss)\n",
    "\n",
    "# Solver\n",
    "solver = tf.train.AdamOptimizer(lr).minimize(vae_loss)\n",
    "\n",
    "# Seesion\n",
    "sess = tf.Session() # Session definition\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for it in range(iteration): # Training process\n",
    "    batch_index = np.random.choice(TR_load.shape[0], size=mb_size, replace=False)\n",
    "    _, loss, KL_error, Recon_error = sess.run([solver, vae_loss, kl_loss_mean, recon_loss_mean], \n",
    "                                              feed_dict={X: TR_load[batch_index]})                                                                                                                       \n",
    "    if it % 1000 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('Loss: {:.4}'. format(loss))\n",
    "        print('KL_error: {}'. format(KL_error))\n",
    "        print('Recon_error: {}'. format(Recon_error))\n",
    "        print()\n",
    "print(\"Training Finished!\") \n",
    "\n",
    "#Save the reslut    \n",
    "saver = tf.train.Saver() \n",
    "saver.save(sess, './filename.chkp') \n",
    "\n",
    "# Session Recover\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()                                                     \n",
    "saver.restore(sess, 'filename.chkp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5Z_dFCR9Ijl"
   },
   "source": [
    "# Load data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIGD5X1rmF3F"
   },
   "outputs": [],
   "source": [
    "# ==========================Generate load data==================================\n",
    "\n",
    "# =======================Function to recover data===============================\n",
    "\n",
    "def Gen_recover_function(Gen_to_be_recovered): \n",
    "    Gen_recovered=Gen_to_be_recovered*(max_Tr_astype-min_Tr_astype)+min_Tr_astype\n",
    "    return Gen_recovered\n",
    "\n",
    "# ========================= Geneter load data ==================================\n",
    "\n",
    "Gen_mu_new, Gen_noisy_new = sess.run([Gen_mu_sample, Gen_noisy_sample], \n",
    "                                             feed_dict={z: np.random.randn(len(Train_load),z_dim)})  \n",
    "    \n",
    "Gen_mu_all = Gen_mu_new   # All generated data mu\n",
    "Gen_mu_recovered_all = Gen_recover_function(Gen_mu_new)   # All generated data mu (rescaled)\n",
    "Gen_noisy_all = Gen_noisy_new # All generated data noisy\n",
    "Gen_noisy_recovered_all = Gen_recover_function(Gen_noisy_new)   # All generated data noisy (rescaled)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation for the file names\n",
    "- filename with \"σ'_auto\": output noise parameter is co-optimised during training (Auto σ')\n",
    "- filename with \"σ'_\"+ a \"number\": output noise parameter is  fixed (Manually set σ')\n",
    "\n",
    "- filename with \"mu\": generated data without noise (noise free)\n",
    "- filename with \"noisy\": generated data with noise (noisy)\n",
    "\n",
    "- filename with \"Time_all\": generated data for 24 hours\n",
    "- filename with \"Time_\"+ a \"number\": generated data for a specific hour of the day\n",
    "\n",
    "- filename with \"nor\": generated data that are not rescaled to the original scale of historical data\n",
    "- filename without \"nor\": generated data that are rescaled to the original scale of historical data\n",
    "\n",
    "- filename with a β number: data generated with that specific β number\n",
    "- filename without a β number: the number of β is 1 by default\n",
    "\n",
    "- filename with \"VAE\" specified: it is a VAE model\n",
    "- filename without \"VAE\" specified: it is a CVAE model by default\n",
    "\n",
    "- filename with \"35\" in the end: a model for 35-dimensional input\n",
    "- filename without \"35\" in the end: a model for 32-dimensional input by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OErVzfvRgwRu"
   },
   "outputs": [],
   "source": [
    "# Save data\n",
    "pd.DataFrame(Gen_mu_recovered_all).to_csv(\"σ'_{:.2f}_Time_all_mu_VAE.csv\".format(σ_manually_set))\n",
    "pd.DataFrame(Gen_noisy_recovered_all).to_csv(\"σ'_{:.2f}_Time_all_noisy_VAE.csv\".format(σ_manually_set))\n",
    "pd.DataFrame(Gen_mu_all).to_csv(\"σ'_{:.2f}_Time_all_mu_nor_VAE.csv\".format(σ_manually_set))\n",
    "pd.DataFrame(Gen_noisy_all).to_csv(\"σ'_{:.2f}_Time_all_noisy_nor_VAE.csv\".format(σ_manually_set))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMA8ubqOj4tujrOkycRk9+I",
   "collapsed_sections": [
    "-xT6IRwfOJOI",
    "scr8FbkxGMBK",
    "KixHk4ecew5P"
   ],
   "name": "13-17_32_VAE.ipynb (Manually set sigma)",
   "provenance": [
    {
     "file_id": "1vXKmHPCfi_Dvzi2TfOaNEuX-qqmzHnGK",
     "timestamp": 1629151753543
    },
    {
     "file_id": "1ZU50rlbIt5HHxBJuixK5D37Fz0M4WkH3",
     "timestamp": 1629113930609
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
