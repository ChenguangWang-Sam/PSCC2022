{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this code\n",
    "\n",
    "- Code author: Chenguang Wang   \n",
    "- Email: c.wang-8@tudelft.nl; samwangchenguang@gmail.com\n",
    "- Affiliation: Delft University of Technology\n",
    "- Project name: Generating multivariate load states using a (conditional) variational autoencoder\n",
    "- Motivation: This is a project for PSCC2022 – Power Systems Computation Conference: [Homepage of the conference](https://pscc2022.pt/)\n",
    "- Aim of this code: Generating 35-dimensional load states using a variational autoencoder with output noise parameter co-optimised (Auto σ')\n",
    "- A preprint is available, and you can check this paper for more details  [Link of the paper](https://arxiv.org/abs/2110.11435)\n",
    "    - Paper authors: Chenguang Wang, Ensieh Sharifnia, Zhi Gao, Simon H. Tindemans, Peter Palensky\n",
    "    - Accepted for publication at PSCC 2022 and a special issue of EPSR\n",
    "    - If you use (parts of) this code, please cite the preprint or published paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skisMfE-8smQ"
   },
   "source": [
    "# Preparation (Library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26933,
     "status": "ok",
     "timestamp": 1632937682400,
     "user": {
      "displayName": "王晨光",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13859634542690233274"
     },
     "user_tz": -120
    },
    "id": "QLbQ8NEBdECl",
    "outputId": "a1e5f8b8-294e-47cf-fc1e-543113d6d5eb"
   },
   "outputs": [],
   "source": [
    "# ==================== Preparation (library) =======================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvAbNvBb81FJ"
   },
   "source": [
    "# Preparation (Data)\n",
    "- Input: Load data for 35 European countries between 2017 and 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2209,
     "status": "ok",
     "timestamp": 1632937695026,
     "user": {
      "displayName": "王晨光",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13859634542690233274"
     },
     "user_tz": -120
    },
    "id": "bu7qExGNWwGR",
    "outputId": "a7706fee-4592-42c6-a1d6-cf84ca806874"
   },
   "outputs": [],
   "source": [
    "# ============================ Get all data ====================================\n",
    "\n",
    "Data = pd.read_csv(\"../Data/17-18_35_conditioned.csv\") # National_load_data_all\n",
    "Train = Data # Note: Different from the other case study (CASE STUDY: EUROPEAN LOAD DATA), we use all data for training in this study (MULTI-AREA ADEQUACY ASSESSMENT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= Separate load data and time label ==========================\n",
    "\n",
    "# Get column location\n",
    "Austria=Data.columns.get_loc('AL_load_actual_entsoe_power_statistics')\n",
    "UK=Data.columns.get_loc('UK_UKM_load_actual_entsoe_power_statistics')\n",
    "hour_sin=Data.columns.get_loc('hour_sin')\n",
    "hour_cos=Data.columns.get_loc('hour_cos')\n",
    "\n",
    "# Get only load data\n",
    "Train_load=Data.iloc[:,Austria:(UK+1)]\n",
    "Train_load.index=range(len(Train_load)) # index rearrange\n",
    "\n",
    "# Get only time label\n",
    "Train_label=Data.iloc[:,hour_sin:(hour_cos+1)]\n",
    "Train_label.index=range(len(Train_label)) # index rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Dara processing for all training and testing data  =============\n",
    "#============================= Traing data processing ==========================\n",
    "\n",
    "# To get \"normalized\" training data\n",
    "Tr = Train_load\n",
    "max_Tr = np.max(Tr, axis = 0) # reserve the maximum value of basic training data\n",
    "min_Tr = np.min(Tr, axis = 0) # reserve the minimum value of basic training data\n",
    "Tr_nor = (Tr-min_Tr)/(max_Tr-min_Tr) # normalized training data\n",
    "\n",
    "mean_Tr = np.mean(Tr, axis = 0) # Mean Value\n",
    "\n",
    "min_Tr_astype = min_Tr.values.astype(np.float32) # Training \n",
    "max_Tr_astype = max_Tr.values.astype(np.float32) # Testing \n",
    "mean_Tr_astyped = mean_Tr.values.astype(np.float32) # Mean\n",
    "\n",
    "# \"Date format\" conversion for all data\n",
    "TR_load = Tr_nor.values.astype(np.float32) # Training Data                                                                                                                \n",
    "TR_label = Train_label.values.astype(np.float32) # Training label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rh10xpPQuROZ"
   },
   "source": [
    "# Preparation (Neuro Network Dessign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 361,
     "status": "ok",
     "timestamp": 1632937701618,
     "user": {
      "displayName": "王晨光",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13859634542690233274"
     },
     "user_tz": -120
    },
    "id": "09e5boHVIgGh"
   },
   "outputs": [],
   "source": [
    "# ================== Preparation (Neuro network dessign) =======================\n",
    "\n",
    "# ========================== Initialization fuction ============================\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "# ===================== Data dimension calcuation ==============================\n",
    "\n",
    "# Dimension of the input\n",
    "X_dim_all=np.shape(Train_load)\n",
    "X_dim = X_dim_all[1]\n",
    "# Dimension of the time label\n",
    "y_dim_all =np.shape(TR_label) \n",
    "y_dim = y_dim_all[1]\n",
    "# Dimension of the hidden layer\n",
    "h_dim_1 = 32\n",
    "# Dimension of the hidden layer\n",
    "h_dim_2 = 24\n",
    "# Dimension of the hidden layer\n",
    "h_dim_3 = 16\n",
    "# Dimension of the latent space\n",
    "z_dim = 8\n",
    "\n",
    "# =============================== Q(z|X,c) =====================================\n",
    "\n",
    "# Data\n",
    "X = tf.placeholder(tf.float32, shape=[None, X_dim])\n",
    "z = tf.placeholder(tf.float32, shape=[None, z_dim])\n",
    "\n",
    "# Layer definitions\n",
    "Q_W1 = tf.Variable(xavier_init([X_dim, h_dim_1]))\n",
    "Q_b1 = tf.Variable(tf.zeros(shape=[h_dim_1]))\n",
    "\n",
    "Q_W2 = tf.Variable(xavier_init([h_dim_1, h_dim_2]))\n",
    "Q_b2 = tf.Variable(tf.zeros(shape=[h_dim_2]))\n",
    "\n",
    "Q_W3 = tf.Variable(xavier_init([h_dim_2, h_dim_3]))\n",
    "Q_b3 = tf.Variable(tf.zeros(shape=[h_dim_3]))\n",
    "\n",
    "Q_W4_mu = tf.Variable(xavier_init([h_dim_3, z_dim]))\n",
    "Q_b4_mu = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "Q_W4_sigma = tf.Variable(xavier_init([h_dim_3, z_dim]))\n",
    "Q_b4_sigma = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "# Network\n",
    "def Q(X): \n",
    "    #inputs = tf.concat(axis=1, values=[X, c]) \n",
    "    h1 = tf.nn.relu(tf.matmul(X, Q_W1) + Q_b1)\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, Q_W2) + Q_b2)\n",
    "    h3 = tf.nn.relu(tf.matmul(h2, Q_W3) + Q_b3)\n",
    "    z_mu = tf.matmul(h3, Q_W4_mu) + Q_b4_mu\n",
    "    z_logvar = tf.matmul(h3, Q_W4_sigma) + Q_b4_sigma\n",
    "    return z_mu, z_logvar\n",
    "\n",
    "# ================Sampling a z from N~(z_mu|tf.exp(z_logvar))===================\n",
    "\n",
    "def sample_z(mu, log_var):\n",
    "    eps = tf.random_normal(shape=tf.shape(mu))\n",
    "    return mu + tf.exp(log_var / 2) * eps\n",
    "\n",
    "# =============================== P(X|z) =======================================\n",
    "\n",
    "# Layer definitions\n",
    "P_W1 = tf.Variable(xavier_init([z_dim , h_dim_3]))  \n",
    "P_b1 = tf.Variable(tf.zeros(shape=[h_dim_3]))\n",
    "\n",
    "P_W2 = tf.Variable(xavier_init([h_dim_3 , h_dim_2]))  \n",
    "P_b2 = tf.Variable(tf.zeros(shape=[h_dim_2]))\n",
    "\n",
    "P_W3 = tf.Variable(xavier_init([h_dim_2 , h_dim_1]))  \n",
    "P_b3 = tf.Variable(tf.zeros(shape=[h_dim_1]))\n",
    "\n",
    "P_W4_mu = tf.Variable(xavier_init([h_dim_1, X_dim]))\n",
    "P_b4_mu = tf.Variable(tf.zeros(shape=[X_dim]))\n",
    "\n",
    "P_W4_logvar = tf.Variable(xavier_init([h_dim_1, X_dim]))\n",
    "P_b4_logvar = tf.Variable(tf.zeros(shape=[X_dim]))\n",
    "\n",
    "\n",
    "# Network\n",
    "def P(z):\n",
    "    h1 = tf.nn.relu(tf.matmul(z, P_W1) + P_b1) \n",
    "    h2 = tf.nn.relu(tf.matmul(h1, P_W2) + P_b2) \n",
    "    h3 = tf.nn.relu(tf.matmul(h2, P_W3) + P_b3) \n",
    "    Gen_mu = tf.matmul(h3, P_W4_mu) + P_b4_mu # Gen_mu : μ_x(z)\n",
    "    Gen_logvar= tf.matmul(h3, P_W4_logvar) + P_b4_logvar # Gen_logvar : log((σ_x(z))**2)   \n",
    "    eps_p = tf.random_normal(shape=tf.shape(Gen_mu)) # epsilon\n",
    "    Gen_noisy = Gen_mu + tf.exp(Gen_logvar/2)*eps_p # Gen_noisy = μ_x(z) + σ_x(z)  * eps_p\n",
    "    return Gen_mu, Gen_logvar , Gen_noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxhv_Dqx9C2r"
   },
   "source": [
    "# Training (VAE; Auto σ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 423031,
     "status": "ok",
     "timestamp": 1632938299066,
     "user": {
      "displayName": "王晨光",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13859634542690233274"
     },
     "user_tz": -120
    },
    "id": "K6xIvQgFgtxO",
    "outputId": "3f01e273-5331-48b4-c5e7-4924a573162b"
   },
   "outputs": [],
   "source": [
    "# ================================ Training ====================================\n",
    "\n",
    "# Training condition: lr (learning rate)=0.0001; batch_size (mb_size) =64;\n",
    "#                     iteration=200,000; σ' Auto\n",
    "\n",
    "#Traning setting \n",
    "mb_size = 64 # Batch size\n",
    "lr = 0.0001 #learning rate\n",
    "β = 10 # weight ratio of kl_loss and recon_loss \n",
    "iteration = 200000 # define the iteration steps\n",
    "\n",
    "# Training functions\n",
    "z_mu, z_logvar = Q(X) # z follows N~(z_μ,z_log(σ^2)) \n",
    "z_sample = sample_z(z_mu, z_logvar) # Sampling z_sample from N~(z_μ, tf.exp(z_log(σ^2)))\n",
    "Gen_mu_train, Gen_logvar_train, Gen_noisy_train = P(z_sample) # generate (hat) x_μ, x_log(σ^2), x given z_sample\n",
    "\n",
    "# Generation functions\n",
    "Gen_mu_sample, Gen_logvar_sample, Gen_noisy_sample = P(z) # Sampling (tilde) x_μ, x_log(σ^2), x given z follows N~(0|1)\n",
    "\n",
    "# Training loss when σ' is automatically generated\n",
    "recon_loss = 0.5 *tf.reduce_sum((Gen_mu_train - X)**2/tf.exp(Gen_logvar_train)+Gen_logvar_train, 1) # recon_error\n",
    "\n",
    "kl_loss  =   β*0.5 * tf.reduce_sum(tf.exp(z_logvar) - 1. - z_logvar + z_mu**2, 1) # KL_error\n",
    "\n",
    "vae_loss =   tf.reduce_mean(recon_loss + kl_loss)+X_dim/2*tf.log(2*(math.pi)) # VAE loss\n",
    "\n",
    "# Monitor what the exact number \n",
    "recon_loss_mean = tf.reduce_mean(recon_loss)+X_dim/2*tf.log(2*(math.pi))\n",
    "\n",
    "kl_loss_mean = tf.reduce_mean(kl_loss)\n",
    "\n",
    "# Solver\n",
    "solver = tf.train.AdamOptimizer(lr).minimize(vae_loss)\n",
    "\n",
    "# Seesion\n",
    "sess = tf.Session() # Session definition\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for it in range(iteration): # Training process\n",
    "    batch_index = np.random.choice(TR_load.shape[0], size=mb_size, replace=False)\n",
    "    _, loss, KL_error, Recon_error = sess.run([solver, vae_loss, kl_loss_mean, recon_loss_mean], \n",
    "                                              feed_dict={X: TR_load[batch_index]})                                                                                                                    \n",
    "    if it % 1000 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('Loss: {:.4}'. format(loss))\n",
    "        print('KL_error: {}'. format(KL_error))\n",
    "        print('Recon_error: {}'. format(Recon_error))\n",
    "        print()\n",
    "print(\"Training Finished!\") \n",
    "\n",
    "#Save the reslut    \n",
    "saver = tf.train.Saver() \n",
    "saver.save(sess, './filename.chkp') \n",
    "\n",
    "# Session Recover\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()                                                     \n",
    "saver.restore(sess, 'filename.chkp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktlE_oE4AkRC"
   },
   "outputs": [],
   "source": [
    "# ==========================Generate load data==================================\n",
    "\n",
    "# =======================Function to recover data===============================\n",
    "\n",
    "def Gen_recover_function(Gen_to_be_recovered): \n",
    "    Gen_recovered=Gen_to_be_recovered*(max_Tr_astype-min_Tr_astype)+min_Tr_astype\n",
    "    return Gen_recovered\n",
    "\n",
    "# How many years' data do you want to generate?\n",
    "year=10\n",
    "\n",
    "# ========================= Geneter load data ==================================\n",
    "\n",
    "Gen_mu_new, Gen_logvar_new, Gen_noisy_new = sess.run([Gen_mu_sample, Gen_logvar_sample, Gen_noisy_sample], \n",
    "                                             feed_dict={z: np.random.randn(len(Train_load)*year,z_dim)})  \n",
    "    \n",
    "Gen_mu_all = Gen_mu_new   # All generated data mu\n",
    "Gen_mu_recovered_all = Gen_recover_function(Gen_mu_new)   # All generated data mu (rescaled)\n",
    "Gen_noisy_all = Gen_noisy_new # All generated data noisy\n",
    "Gen_noisy_recovered_all = Gen_recover_function(Gen_noisy_new)   # All generated data noisy (rescaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation for the file names\n",
    "- filename with \"σ'_auto\": output noise parameter is co-optimised during training (Auto σ')\n",
    "- filename with \"σ'_\"+ a \"number\": output noise parameter is  fixed (Manually set σ')\n",
    "\n",
    "- filename with \"mu\": generated data without noise (noise free)\n",
    "- filename with \"noisy\": generated data with noise (noisy)\n",
    "\n",
    "- filename with \"Time_all\": generated data for 24 hours\n",
    "- filename with \"Time_\"+ a \"number\": generated data for a specific hour of the day\n",
    "\n",
    "- filename with \"nor\": generated data that are not rescaled to the original scale of historical data\n",
    "- filename without \"nor\": generated data that are rescaled to the original scale of historical data\n",
    "\n",
    "- filename with a β number: data generated with that specific β number\n",
    "- filename without a β number: the number of β is 1 by default\n",
    "\n",
    "- filename with \"VAE\" specified: it is a VAE model\n",
    "- filename without \"VAE\" specified: it is a CVAE model by default\n",
    "\n",
    "- filename with \"35\" in the end: a model for 35-dimensional input\n",
    "- filename without \"35\" in the end: a model for 32-dimensional input by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 80,
     "status": "ok",
     "timestamp": 1631120196303,
     "user": {
      "displayName": "王晨光",
      "photoUrl": "",
      "userId": "13859634542690233274"
     },
     "user_tz": -120
    },
    "id": "foekJfRECAz2",
    "outputId": "a49f44b5-faa3-4e4e-830c-7fcab2429e79"
   },
   "outputs": [],
   "source": [
    "# Save data\n",
    "pd.DataFrame(Gen_mu_recovered_all).to_csv(\"σ'_auto_Time_all_mu_VAE_β={:.1f}_35.csv\".format(β))\n",
    "pd.DataFrame(Gen_noisy_recovered_all).to_csv(\"σ'_auto_Time_all_noisy_VAE_β={:.1f}_35.csv\".format(β))\n",
    "pd.DataFrame(Gen_mu_all).to_csv(\"σ'_auto_Time_all_mu_nor_VAE_β={:.1f}_35.csv\".format(β))\n",
    "pd.DataFrame(Gen_noisy_all).to_csv(\"σ'_auto_Time_all_noisy_nor_VAE_β={:.1f}_35.csv\".format(β))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO1wAanpSmOzbzhxBHrQr0B",
   "collapsed_sections": [
    "skisMfE-8smQ",
    "yvAbNvBb81FJ",
    "Rh10xpPQuROZ"
   ],
   "name": "17-18_35(AL)_VAE_try_national_load.ipynb (Automatically adding noise sigma at the output side)",
   "provenance": [
    {
     "file_id": "1cOsK5hW9gd_Z5JnmxxQmoDekcLtyZoRq",
     "timestamp": 1625653283535
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
